{
  "best_global_step": 5000,
  "best_metric": 91.36546184738957,
  "best_model_checkpoint": "output/checkpoint-5000",
  "epoch": 6.68494983277592,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.033444816053511704,
      "grad_norm": 5.141410827636719,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 3.2105,
      "step": 25
    },
    {
      "epoch": 0.06688963210702341,
      "grad_norm": 5.759732723236084,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 3.136,
      "step": 50
    },
    {
      "epoch": 0.10033444816053512,
      "grad_norm": 5.34028434753418,
      "learning_rate": 7.4e-06,
      "loss": 2.9037,
      "step": 75
    },
    {
      "epoch": 0.13377926421404682,
      "grad_norm": 5.477591037750244,
      "learning_rate": 9.900000000000002e-06,
      "loss": 2.6642,
      "step": 100
    },
    {
      "epoch": 0.16722408026755853,
      "grad_norm": 4.576887607574463,
      "learning_rate": 1.23e-05,
      "loss": 2.3594,
      "step": 125
    },
    {
      "epoch": 0.20066889632107024,
      "grad_norm": 4.523379325866699,
      "learning_rate": 1.48e-05,
      "loss": 2.1519,
      "step": 150
    },
    {
      "epoch": 0.23411371237458195,
      "grad_norm": 5.056726455688477,
      "learning_rate": 1.73e-05,
      "loss": 2.1034,
      "step": 175
    },
    {
      "epoch": 0.26755852842809363,
      "grad_norm": 5.7585906982421875,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 1.9055,
      "step": 200
    },
    {
      "epoch": 0.3010033444816054,
      "grad_norm": 5.833674907684326,
      "learning_rate": 2.23e-05,
      "loss": 1.7493,
      "step": 225
    },
    {
      "epoch": 0.33444816053511706,
      "grad_norm": 5.506804466247559,
      "learning_rate": 2.48e-05,
      "loss": 1.7122,
      "step": 250
    },
    {
      "epoch": 0.36789297658862874,
      "grad_norm": 7.186049461364746,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 1.6242,
      "step": 275
    },
    {
      "epoch": 0.4013377926421405,
      "grad_norm": 6.311936855316162,
      "learning_rate": 2.98e-05,
      "loss": 1.5844,
      "step": 300
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 6.5604400634765625,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 1.556,
      "step": 325
    },
    {
      "epoch": 0.4682274247491639,
      "grad_norm": 5.177771091461182,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 1.4311,
      "step": 350
    },
    {
      "epoch": 0.5016722408026756,
      "grad_norm": 6.256592273712158,
      "learning_rate": 3.72e-05,
      "loss": 1.4889,
      "step": 375
    },
    {
      "epoch": 0.5351170568561873,
      "grad_norm": 5.943593978881836,
      "learning_rate": 3.97e-05,
      "loss": 1.4827,
      "step": 400
    },
    {
      "epoch": 0.568561872909699,
      "grad_norm": 5.094482898712158,
      "learning_rate": 4.22e-05,
      "loss": 1.3653,
      "step": 425
    },
    {
      "epoch": 0.6020066889632107,
      "grad_norm": 8.217129707336426,
      "learning_rate": 4.47e-05,
      "loss": 1.2984,
      "step": 450
    },
    {
      "epoch": 0.6354515050167224,
      "grad_norm": 6.144221305847168,
      "learning_rate": 4.72e-05,
      "loss": 1.3792,
      "step": 475
    },
    {
      "epoch": 0.6688963210702341,
      "grad_norm": 5.766080379486084,
      "learning_rate": 4.97e-05,
      "loss": 1.3097,
      "step": 500
    },
    {
      "epoch": 0.6688963210702341,
      "eval_loss": 1.682694435119629,
      "eval_runtime": 294.7188,
      "eval_samples_per_second": 5.069,
      "eval_steps_per_second": 0.635,
      "eval_wer": 96.18473895582329,
      "step": 500
    },
    {
      "epoch": 0.7023411371237458,
      "grad_norm": 5.644071578979492,
      "learning_rate": 4.999987595435707e-05,
      "loss": 1.2634,
      "step": 525
    },
    {
      "epoch": 0.7357859531772575,
      "grad_norm": 6.720248699188232,
      "learning_rate": 4.999943385120328e-05,
      "loss": 1.1693,
      "step": 550
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 6.396265029907227,
      "learning_rate": 4.9998671389568295e-05,
      "loss": 1.2785,
      "step": 575
    },
    {
      "epoch": 0.802675585284281,
      "grad_norm": 6.0996317863464355,
      "learning_rate": 4.999758857922278e-05,
      "loss": 1.2276,
      "step": 600
    },
    {
      "epoch": 0.8361204013377926,
      "grad_norm": 7.375853061676025,
      "learning_rate": 4.9996185434042545e-05,
      "loss": 1.2665,
      "step": 625
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 5.676890850067139,
      "learning_rate": 4.9994461972008366e-05,
      "loss": 1.2005,
      "step": 650
    },
    {
      "epoch": 0.903010033444816,
      "grad_norm": 6.1630072593688965,
      "learning_rate": 4.999241821520579e-05,
      "loss": 1.191,
      "step": 675
    },
    {
      "epoch": 0.9364548494983278,
      "grad_norm": 6.626178741455078,
      "learning_rate": 4.9990054189824795e-05,
      "loss": 1.1476,
      "step": 700
    },
    {
      "epoch": 0.9698996655518395,
      "grad_norm": 7.028685092926025,
      "learning_rate": 4.998736992615949e-05,
      "loss": 1.2195,
      "step": 725
    },
    {
      "epoch": 1.002675585284281,
      "grad_norm": 5.067840576171875,
      "learning_rate": 4.998436545860772e-05,
      "loss": 1.1388,
      "step": 750
    },
    {
      "epoch": 1.0361204013377927,
      "grad_norm": 6.195781230926514,
      "learning_rate": 4.998104082567062e-05,
      "loss": 1.167,
      "step": 775
    },
    {
      "epoch": 1.0695652173913044,
      "grad_norm": 5.904269218444824,
      "learning_rate": 4.997739606995213e-05,
      "loss": 1.082,
      "step": 800
    },
    {
      "epoch": 1.103010033444816,
      "grad_norm": 6.914648056030273,
      "learning_rate": 4.997343123815844e-05,
      "loss": 1.1449,
      "step": 825
    },
    {
      "epoch": 1.1364548494983278,
      "grad_norm": 4.768416404724121,
      "learning_rate": 4.996914638109741e-05,
      "loss": 1.133,
      "step": 850
    },
    {
      "epoch": 1.1698996655518394,
      "grad_norm": 5.886300086975098,
      "learning_rate": 4.996454155367789e-05,
      "loss": 1.1093,
      "step": 875
    },
    {
      "epoch": 1.2033444816053511,
      "grad_norm": 6.900811672210693,
      "learning_rate": 4.9959616814909046e-05,
      "loss": 1.1583,
      "step": 900
    },
    {
      "epoch": 1.2367892976588628,
      "grad_norm": 5.7072319984436035,
      "learning_rate": 4.995437222789956e-05,
      "loss": 1.0534,
      "step": 925
    },
    {
      "epoch": 1.2702341137123745,
      "grad_norm": 6.185003757476807,
      "learning_rate": 4.9948807859856895e-05,
      "loss": 1.1079,
      "step": 950
    },
    {
      "epoch": 1.3036789297658862,
      "grad_norm": 6.169313907623291,
      "learning_rate": 4.9942923782086346e-05,
      "loss": 1.0663,
      "step": 975
    },
    {
      "epoch": 1.3371237458193979,
      "grad_norm": 5.825844764709473,
      "learning_rate": 4.99367200699902e-05,
      "loss": 1.048,
      "step": 1000
    },
    {
      "epoch": 1.3371237458193979,
      "eval_loss": 1.4659136533737183,
      "eval_runtime": 279.3415,
      "eval_samples_per_second": 5.348,
      "eval_steps_per_second": 0.669,
      "eval_wer": 94.77911646586345,
      "step": 1000
    },
    {
      "epoch": 1.3705685618729098,
      "grad_norm": 5.184537887573242,
      "learning_rate": 4.993019680306672e-05,
      "loss": 0.9941,
      "step": 1025
    },
    {
      "epoch": 1.4040133779264214,
      "grad_norm": 5.771644115447998,
      "learning_rate": 4.992335406490917e-05,
      "loss": 1.002,
      "step": 1050
    },
    {
      "epoch": 1.4374581939799331,
      "grad_norm": 6.5696611404418945,
      "learning_rate": 4.991619194320468e-05,
      "loss": 1.0538,
      "step": 1075
    },
    {
      "epoch": 1.4709030100334448,
      "grad_norm": 4.987785339355469,
      "learning_rate": 4.990871052973322e-05,
      "loss": 1.0456,
      "step": 1100
    },
    {
      "epoch": 1.5043478260869565,
      "grad_norm": 5.580188274383545,
      "learning_rate": 4.9900909920366295e-05,
      "loss": 1.0815,
      "step": 1125
    },
    {
      "epoch": 1.5377926421404682,
      "grad_norm": 4.828797817230225,
      "learning_rate": 4.989279021506584e-05,
      "loss": 1.025,
      "step": 1150
    },
    {
      "epoch": 1.57123745819398,
      "grad_norm": 4.676807880401611,
      "learning_rate": 4.9884351517882875e-05,
      "loss": 1.0468,
      "step": 1175
    },
    {
      "epoch": 1.6046822742474918,
      "grad_norm": 5.4909281730651855,
      "learning_rate": 4.9875593936956176e-05,
      "loss": 1.0495,
      "step": 1200
    },
    {
      "epoch": 1.6381270903010035,
      "grad_norm": 5.183631896972656,
      "learning_rate": 4.986651758451089e-05,
      "loss": 1.0322,
      "step": 1225
    },
    {
      "epoch": 1.6715719063545151,
      "grad_norm": 3.991508722305298,
      "learning_rate": 4.985712257685711e-05,
      "loss": 0.9958,
      "step": 1250
    },
    {
      "epoch": 1.7050167224080268,
      "grad_norm": 4.734073638916016,
      "learning_rate": 4.984740903438839e-05,
      "loss": 1.0361,
      "step": 1275
    },
    {
      "epoch": 1.7384615384615385,
      "grad_norm": 5.400135040283203,
      "learning_rate": 4.9837377081580164e-05,
      "loss": 1.0041,
      "step": 1300
    },
    {
      "epoch": 1.7719063545150502,
      "grad_norm": 5.937768459320068,
      "learning_rate": 4.982702684698819e-05,
      "loss": 1.0153,
      "step": 1325
    },
    {
      "epoch": 1.8053511705685619,
      "grad_norm": 4.395820617675781,
      "learning_rate": 4.981635846324689e-05,
      "loss": 0.9806,
      "step": 1350
    },
    {
      "epoch": 1.8387959866220736,
      "grad_norm": 4.887124538421631,
      "learning_rate": 4.980537206706767e-05,
      "loss": 1.0476,
      "step": 1375
    },
    {
      "epoch": 1.8722408026755852,
      "grad_norm": 4.331875801086426,
      "learning_rate": 4.9794067799237095e-05,
      "loss": 0.9882,
      "step": 1400
    },
    {
      "epoch": 1.905685618729097,
      "grad_norm": 3.86479115486145,
      "learning_rate": 4.978244580461518e-05,
      "loss": 0.9243,
      "step": 1425
    },
    {
      "epoch": 1.9391304347826086,
      "grad_norm": 5.1902055740356445,
      "learning_rate": 4.977050623213348e-05,
      "loss": 0.968,
      "step": 1450
    },
    {
      "epoch": 1.9725752508361203,
      "grad_norm": 4.654480457305908,
      "learning_rate": 4.9758249234793204e-05,
      "loss": 0.9131,
      "step": 1475
    },
    {
      "epoch": 2.005351170568562,
      "grad_norm": 5.565362930297852,
      "learning_rate": 4.9745674969663216e-05,
      "loss": 0.8993,
      "step": 1500
    },
    {
      "epoch": 2.005351170568562,
      "eval_loss": 1.342382788658142,
      "eval_runtime": 270.5133,
      "eval_samples_per_second": 5.523,
      "eval_steps_per_second": 0.691,
      "eval_wer": 94.44444444444444,
      "step": 1500
    },
    {
      "epoch": 2.0387959866220737,
      "grad_norm": 4.196619987487793,
      "learning_rate": 4.973278359787806e-05,
      "loss": 0.9598,
      "step": 1525
    },
    {
      "epoch": 2.0722408026755854,
      "grad_norm": 4.634378433227539,
      "learning_rate": 4.9719575284635904e-05,
      "loss": 0.8749,
      "step": 1550
    },
    {
      "epoch": 2.105685618729097,
      "grad_norm": 5.040216445922852,
      "learning_rate": 4.970605019919638e-05,
      "loss": 0.9383,
      "step": 1575
    },
    {
      "epoch": 2.139130434782609,
      "grad_norm": 4.947728157043457,
      "learning_rate": 4.9692208514878444e-05,
      "loss": 0.9394,
      "step": 1600
    },
    {
      "epoch": 2.1725752508361205,
      "grad_norm": 6.435327529907227,
      "learning_rate": 4.967805040905816e-05,
      "loss": 0.9211,
      "step": 1625
    },
    {
      "epoch": 2.206020066889632,
      "grad_norm": 4.447059154510498,
      "learning_rate": 4.9663576063166395e-05,
      "loss": 0.8386,
      "step": 1650
    },
    {
      "epoch": 2.239464882943144,
      "grad_norm": 4.186105251312256,
      "learning_rate": 4.964878566268654e-05,
      "loss": 0.8599,
      "step": 1675
    },
    {
      "epoch": 2.2729096989966555,
      "grad_norm": 4.607281684875488,
      "learning_rate": 4.963367939715209e-05,
      "loss": 0.8968,
      "step": 1700
    },
    {
      "epoch": 2.306354515050167,
      "grad_norm": 4.619268894195557,
      "learning_rate": 4.9618257460144246e-05,
      "loss": 0.8754,
      "step": 1725
    },
    {
      "epoch": 2.339799331103679,
      "grad_norm": 4.791032314300537,
      "learning_rate": 4.96025200492894e-05,
      "loss": 0.9074,
      "step": 1750
    },
    {
      "epoch": 2.3732441471571906,
      "grad_norm": 5.04008150100708,
      "learning_rate": 4.958646736625666e-05,
      "loss": 0.9779,
      "step": 1775
    },
    {
      "epoch": 2.4066889632107022,
      "grad_norm": 5.015810966491699,
      "learning_rate": 4.957009961675521e-05,
      "loss": 0.8572,
      "step": 1800
    },
    {
      "epoch": 2.440133779264214,
      "grad_norm": 5.675069332122803,
      "learning_rate": 4.9553417010531674e-05,
      "loss": 0.8892,
      "step": 1825
    },
    {
      "epoch": 2.4735785953177256,
      "grad_norm": 4.864439010620117,
      "learning_rate": 4.95364197613675e-05,
      "loss": 0.8435,
      "step": 1850
    },
    {
      "epoch": 2.5070234113712373,
      "grad_norm": 4.717472553253174,
      "learning_rate": 4.9519108087076105e-05,
      "loss": 0.8756,
      "step": 1875
    },
    {
      "epoch": 2.540468227424749,
      "grad_norm": 3.540968418121338,
      "learning_rate": 4.950148220950021e-05,
      "loss": 0.9409,
      "step": 1900
    },
    {
      "epoch": 2.573913043478261,
      "grad_norm": 5.696862697601318,
      "learning_rate": 4.948354235450888e-05,
      "loss": 0.908,
      "step": 1925
    },
    {
      "epoch": 2.6073578595317723,
      "grad_norm": 3.8414275646209717,
      "learning_rate": 4.946528875199472e-05,
      "loss": 0.8958,
      "step": 1950
    },
    {
      "epoch": 2.6408026755852845,
      "grad_norm": 4.0627312660217285,
      "learning_rate": 4.9446721635870886e-05,
      "loss": 0.8791,
      "step": 1975
    },
    {
      "epoch": 2.6742474916387957,
      "grad_norm": 5.150545597076416,
      "learning_rate": 4.9427841244068085e-05,
      "loss": 0.8173,
      "step": 2000
    },
    {
      "epoch": 2.6742474916387957,
      "eval_loss": 1.2928171157836914,
      "eval_runtime": 248.3337,
      "eval_samples_per_second": 6.016,
      "eval_steps_per_second": 0.753,
      "eval_wer": 92.7041499330656,
      "step": 2000
    },
    {
      "epoch": 2.707692307692308,
      "grad_norm": 4.474879264831543,
      "learning_rate": 4.940864781853154e-05,
      "loss": 0.85,
      "step": 2025
    },
    {
      "epoch": 2.7411371237458195,
      "grad_norm": 5.001134395599365,
      "learning_rate": 4.938914160521789e-05,
      "loss": 0.8326,
      "step": 2050
    },
    {
      "epoch": 2.774581939799331,
      "grad_norm": 4.468257427215576,
      "learning_rate": 4.936932285409204e-05,
      "loss": 0.8755,
      "step": 2075
    },
    {
      "epoch": 2.808026755852843,
      "grad_norm": 4.425261497497559,
      "learning_rate": 4.934919181912393e-05,
      "loss": 0.8787,
      "step": 2100
    },
    {
      "epoch": 2.8414715719063546,
      "grad_norm": 4.020277976989746,
      "learning_rate": 4.932874875828531e-05,
      "loss": 0.89,
      "step": 2125
    },
    {
      "epoch": 2.8749163879598663,
      "grad_norm": 4.761152267456055,
      "learning_rate": 4.930799393354645e-05,
      "loss": 0.9119,
      "step": 2150
    },
    {
      "epoch": 2.908361204013378,
      "grad_norm": 5.135113716125488,
      "learning_rate": 4.928692761087271e-05,
      "loss": 0.8622,
      "step": 2175
    },
    {
      "epoch": 2.9418060200668896,
      "grad_norm": 4.139658451080322,
      "learning_rate": 4.926555006022123e-05,
      "loss": 0.8978,
      "step": 2200
    },
    {
      "epoch": 2.9752508361204013,
      "grad_norm": 5.994433403015137,
      "learning_rate": 4.92438615555374e-05,
      "loss": 0.8495,
      "step": 2225
    },
    {
      "epoch": 3.0080267558528426,
      "grad_norm": 4.130894660949707,
      "learning_rate": 4.922186237475135e-05,
      "loss": 0.8522,
      "step": 2250
    },
    {
      "epoch": 3.0414715719063543,
      "grad_norm": 4.3999223709106445,
      "learning_rate": 4.919955279977445e-05,
      "loss": 0.7452,
      "step": 2275
    },
    {
      "epoch": 3.074916387959866,
      "grad_norm": 4.211801052093506,
      "learning_rate": 4.917693311649563e-05,
      "loss": 0.7753,
      "step": 2300
    },
    {
      "epoch": 3.108361204013378,
      "grad_norm": 3.812126874923706,
      "learning_rate": 4.915400361477775e-05,
      "loss": 0.7058,
      "step": 2325
    },
    {
      "epoch": 3.14180602006689,
      "grad_norm": 3.8701584339141846,
      "learning_rate": 4.913076458845388e-05,
      "loss": 0.7523,
      "step": 2350
    },
    {
      "epoch": 3.1752508361204015,
      "grad_norm": 5.329899311065674,
      "learning_rate": 4.9107216335323525e-05,
      "loss": 0.8242,
      "step": 2375
    },
    {
      "epoch": 3.208695652173913,
      "grad_norm": 5.073534965515137,
      "learning_rate": 4.908335915714883e-05,
      "loss": 0.7934,
      "step": 2400
    },
    {
      "epoch": 3.242140468227425,
      "grad_norm": 4.760059356689453,
      "learning_rate": 4.905919335965069e-05,
      "loss": 0.8426,
      "step": 2425
    },
    {
      "epoch": 3.2755852842809365,
      "grad_norm": 4.529475688934326,
      "learning_rate": 4.9034719252504843e-05,
      "loss": 0.7744,
      "step": 2450
    },
    {
      "epoch": 3.309030100334448,
      "grad_norm": 4.064265727996826,
      "learning_rate": 4.900993714933791e-05,
      "loss": 0.7655,
      "step": 2475
    },
    {
      "epoch": 3.34247491638796,
      "grad_norm": 4.016871452331543,
      "learning_rate": 4.898484736772334e-05,
      "loss": 0.8281,
      "step": 2500
    },
    {
      "epoch": 3.34247491638796,
      "eval_loss": 1.2469171285629272,
      "eval_runtime": 253.3142,
      "eval_samples_per_second": 5.898,
      "eval_steps_per_second": 0.738,
      "eval_wer": 92.7710843373494,
      "step": 2500
    },
    {
      "epoch": 3.3759197324414716,
      "grad_norm": 5.150118350982666,
      "learning_rate": 4.8959450229177416e-05,
      "loss": 0.834,
      "step": 2525
    },
    {
      "epoch": 3.4093645484949833,
      "grad_norm": 4.409848213195801,
      "learning_rate": 4.893374605915505e-05,
      "loss": 0.7821,
      "step": 2550
    },
    {
      "epoch": 3.442809364548495,
      "grad_norm": 3.7949180603027344,
      "learning_rate": 4.890773518704563e-05,
      "loss": 0.7696,
      "step": 2575
    },
    {
      "epoch": 3.4762541806020066,
      "grad_norm": 4.879249095916748,
      "learning_rate": 4.8881417946168886e-05,
      "loss": 0.8188,
      "step": 2600
    },
    {
      "epoch": 3.5096989966555183,
      "grad_norm": 3.892605781555176,
      "learning_rate": 4.885479467377046e-05,
      "loss": 0.772,
      "step": 2625
    },
    {
      "epoch": 3.54314381270903,
      "grad_norm": 4.984879493713379,
      "learning_rate": 4.882786571101777e-05,
      "loss": 0.8018,
      "step": 2650
    },
    {
      "epoch": 3.5765886287625417,
      "grad_norm": 4.97135066986084,
      "learning_rate": 4.880063140299547e-05,
      "loss": 0.7794,
      "step": 2675
    },
    {
      "epoch": 3.6100334448160534,
      "grad_norm": 4.490660190582275,
      "learning_rate": 4.8773092098701124e-05,
      "loss": 0.7801,
      "step": 2700
    },
    {
      "epoch": 3.643478260869565,
      "grad_norm": 3.672907829284668,
      "learning_rate": 4.874524815104072e-05,
      "loss": 0.7657,
      "step": 2725
    },
    {
      "epoch": 3.676923076923077,
      "grad_norm": 3.710757255554199,
      "learning_rate": 4.8717099916824124e-05,
      "loss": 0.7584,
      "step": 2750
    },
    {
      "epoch": 3.7103678929765884,
      "grad_norm": 5.1799187660217285,
      "learning_rate": 4.868864775676053e-05,
      "loss": 0.7943,
      "step": 2775
    },
    {
      "epoch": 3.7438127090301005,
      "grad_norm": 4.822722911834717,
      "learning_rate": 4.865989203545382e-05,
      "loss": 0.7659,
      "step": 2800
    },
    {
      "epoch": 3.777257525083612,
      "grad_norm": 3.249905586242676,
      "learning_rate": 4.863083312139792e-05,
      "loss": 0.7501,
      "step": 2825
    },
    {
      "epoch": 3.810702341137124,
      "grad_norm": 4.44489049911499,
      "learning_rate": 4.860147138697203e-05,
      "loss": 0.7848,
      "step": 2850
    },
    {
      "epoch": 3.8441471571906356,
      "grad_norm": 4.300877571105957,
      "learning_rate": 4.857180720843591e-05,
      "loss": 0.8095,
      "step": 2875
    },
    {
      "epoch": 3.8775919732441473,
      "grad_norm": 4.092829704284668,
      "learning_rate": 4.854184096592501e-05,
      "loss": 0.7667,
      "step": 2900
    },
    {
      "epoch": 3.911036789297659,
      "grad_norm": 4.148807048797607,
      "learning_rate": 4.8511573043445625e-05,
      "loss": 0.7807,
      "step": 2925
    },
    {
      "epoch": 3.9444816053511706,
      "grad_norm": 4.2377028465271,
      "learning_rate": 4.8481003828869966e-05,
      "loss": 0.7274,
      "step": 2950
    },
    {
      "epoch": 3.9779264214046823,
      "grad_norm": 4.7098541259765625,
      "learning_rate": 4.845013371393119e-05,
      "loss": 0.7495,
      "step": 2975
    },
    {
      "epoch": 4.010702341137124,
      "grad_norm": 3.8755414485931396,
      "learning_rate": 4.841896309421837e-05,
      "loss": 0.7385,
      "step": 3000
    },
    {
      "epoch": 4.010702341137124,
      "eval_loss": 1.1996766328811646,
      "eval_runtime": 250.2866,
      "eval_samples_per_second": 5.969,
      "eval_steps_per_second": 0.747,
      "eval_wer": 92.43641231593038,
      "step": 3000
    },
    {
      "epoch": 4.044147157190635,
      "grad_norm": 3.6144654750823975,
      "learning_rate": 4.838749236917147e-05,
      "loss": 0.689,
      "step": 3025
    },
    {
      "epoch": 4.0775919732441475,
      "grad_norm": 3.3424298763275146,
      "learning_rate": 4.8355721942076155e-05,
      "loss": 0.6782,
      "step": 3050
    },
    {
      "epoch": 4.111036789297659,
      "grad_norm": 4.445319652557373,
      "learning_rate": 4.832365222005868e-05,
      "loss": 0.662,
      "step": 3075
    },
    {
      "epoch": 4.144481605351171,
      "grad_norm": 3.997802734375,
      "learning_rate": 4.829128361408064e-05,
      "loss": 0.6686,
      "step": 3100
    },
    {
      "epoch": 4.177926421404682,
      "grad_norm": 5.130672454833984,
      "learning_rate": 4.8258616538933735e-05,
      "loss": 0.7018,
      "step": 3125
    },
    {
      "epoch": 4.211371237458194,
      "grad_norm": 3.9345054626464844,
      "learning_rate": 4.822565141323443e-05,
      "loss": 0.7474,
      "step": 3150
    },
    {
      "epoch": 4.244816053511705,
      "grad_norm": 4.041449069976807,
      "learning_rate": 4.819238865941859e-05,
      "loss": 0.6781,
      "step": 3175
    },
    {
      "epoch": 4.278260869565218,
      "grad_norm": 5.026670932769775,
      "learning_rate": 4.815882870373608e-05,
      "loss": 0.7161,
      "step": 3200
    },
    {
      "epoch": 4.311705685618729,
      "grad_norm": 5.035859107971191,
      "learning_rate": 4.81249719762453e-05,
      "loss": 0.7361,
      "step": 3225
    },
    {
      "epoch": 4.345150501672241,
      "grad_norm": 4.1496500968933105,
      "learning_rate": 4.8090818910807663e-05,
      "loss": 0.7314,
      "step": 3250
    },
    {
      "epoch": 4.378595317725752,
      "grad_norm": 4.9990057945251465,
      "learning_rate": 4.805636994508206e-05,
      "loss": 0.6751,
      "step": 3275
    },
    {
      "epoch": 4.412040133779264,
      "grad_norm": 4.453446865081787,
      "learning_rate": 4.80216255205192e-05,
      "loss": 0.6941,
      "step": 3300
    },
    {
      "epoch": 4.4454849498327755,
      "grad_norm": 3.9764249324798584,
      "learning_rate": 4.7986586082356025e-05,
      "loss": 0.7111,
      "step": 3325
    },
    {
      "epoch": 4.478929765886288,
      "grad_norm": 4.235662937164307,
      "learning_rate": 4.795125207960994e-05,
      "loss": 0.6987,
      "step": 3350
    },
    {
      "epoch": 4.512374581939799,
      "grad_norm": 4.8106584548950195,
      "learning_rate": 4.791562396507311e-05,
      "loss": 0.715,
      "step": 3375
    },
    {
      "epoch": 4.545819397993311,
      "grad_norm": 3.774751901626587,
      "learning_rate": 4.787970219530661e-05,
      "loss": 0.6819,
      "step": 3400
    },
    {
      "epoch": 4.579264214046823,
      "grad_norm": 3.40461802482605,
      "learning_rate": 4.784348723063461e-05,
      "loss": 0.6634,
      "step": 3425
    },
    {
      "epoch": 4.612709030100334,
      "grad_norm": 4.531749725341797,
      "learning_rate": 4.780697953513847e-05,
      "loss": 0.6598,
      "step": 3450
    },
    {
      "epoch": 4.6461538461538465,
      "grad_norm": 4.365032196044922,
      "learning_rate": 4.7770179576650775e-05,
      "loss": 0.6596,
      "step": 3475
    },
    {
      "epoch": 4.679598662207358,
      "grad_norm": 3.6708903312683105,
      "learning_rate": 4.773308782674935e-05,
      "loss": 0.6932,
      "step": 3500
    },
    {
      "epoch": 4.679598662207358,
      "eval_loss": 1.179548978805542,
      "eval_runtime": 231.8415,
      "eval_samples_per_second": 6.444,
      "eval_steps_per_second": 0.807,
      "eval_wer": 92.03480589022757,
      "step": 3500
    },
    {
      "epoch": 4.71304347826087,
      "grad_norm": 4.5659379959106445,
      "learning_rate": 4.769570476075124e-05,
      "loss": 0.7071,
      "step": 3525
    },
    {
      "epoch": 4.746488294314381,
      "grad_norm": 3.507502794265747,
      "learning_rate": 4.765803085770657e-05,
      "loss": 0.678,
      "step": 3550
    },
    {
      "epoch": 4.779933110367893,
      "grad_norm": 4.4440412521362305,
      "learning_rate": 4.762006660039245e-05,
      "loss": 0.7137,
      "step": 3575
    },
    {
      "epoch": 4.8133779264214045,
      "grad_norm": 3.9268274307250977,
      "learning_rate": 4.7581812475306784e-05,
      "loss": 0.7012,
      "step": 3600
    },
    {
      "epoch": 4.846822742474917,
      "grad_norm": 4.345775604248047,
      "learning_rate": 4.754326897266199e-05,
      "loss": 0.7188,
      "step": 3625
    },
    {
      "epoch": 4.880267558528428,
      "grad_norm": 3.82576060295105,
      "learning_rate": 4.750443658637879e-05,
      "loss": 0.7226,
      "step": 3650
    },
    {
      "epoch": 4.91371237458194,
      "grad_norm": 4.240094184875488,
      "learning_rate": 4.7465315814079803e-05,
      "loss": 0.6169,
      "step": 3675
    },
    {
      "epoch": 4.947157190635451,
      "grad_norm": 5.277974605560303,
      "learning_rate": 4.742590715708325e-05,
      "loss": 0.6984,
      "step": 3700
    },
    {
      "epoch": 4.980602006688963,
      "grad_norm": 3.8146438598632812,
      "learning_rate": 4.738621112039645e-05,
      "loss": 0.6447,
      "step": 3725
    },
    {
      "epoch": 5.013377926421405,
      "grad_norm": 3.5689516067504883,
      "learning_rate": 4.734622821270942e-05,
      "loss": 0.7146,
      "step": 3750
    },
    {
      "epoch": 5.046822742474917,
      "grad_norm": 4.285063743591309,
      "learning_rate": 4.73059589463883e-05,
      "loss": 0.5867,
      "step": 3775
    },
    {
      "epoch": 5.080267558528428,
      "grad_norm": 4.037808418273926,
      "learning_rate": 4.726540383746881e-05,
      "loss": 0.5929,
      "step": 3800
    },
    {
      "epoch": 5.11371237458194,
      "grad_norm": 2.8675239086151123,
      "learning_rate": 4.7224563405649656e-05,
      "loss": 0.6265,
      "step": 3825
    },
    {
      "epoch": 5.147157190635451,
      "grad_norm": 4.700009346008301,
      "learning_rate": 4.7183438174285835e-05,
      "loss": 0.603,
      "step": 3850
    },
    {
      "epoch": 5.1806020066889635,
      "grad_norm": 3.6663095951080322,
      "learning_rate": 4.714202867038193e-05,
      "loss": 0.6332,
      "step": 3875
    },
    {
      "epoch": 5.214046822742475,
      "grad_norm": 2.895904302597046,
      "learning_rate": 4.7100335424585405e-05,
      "loss": 0.615,
      "step": 3900
    },
    {
      "epoch": 5.247491638795987,
      "grad_norm": 4.648277282714844,
      "learning_rate": 4.7058358971179756e-05,
      "loss": 0.6358,
      "step": 3925
    },
    {
      "epoch": 5.280936454849498,
      "grad_norm": 3.043627977371216,
      "learning_rate": 4.701609984807768e-05,
      "loss": 0.6374,
      "step": 3950
    },
    {
      "epoch": 5.31438127090301,
      "grad_norm": 3.1984894275665283,
      "learning_rate": 4.697355859681417e-05,
      "loss": 0.6251,
      "step": 3975
    },
    {
      "epoch": 5.3478260869565215,
      "grad_norm": 3.491010904312134,
      "learning_rate": 4.69307357625396e-05,
      "loss": 0.6513,
      "step": 4000
    },
    {
      "epoch": 5.3478260869565215,
      "eval_loss": 1.1585596799850464,
      "eval_runtime": 232.7598,
      "eval_samples_per_second": 6.419,
      "eval_steps_per_second": 0.803,
      "eval_wer": 92.10174029451139,
      "step": 4000
    },
    {
      "epoch": 5.381270903010034,
      "grad_norm": 3.3746883869171143,
      "learning_rate": 4.688763189401273e-05,
      "loss": 0.6317,
      "step": 4025
    },
    {
      "epoch": 5.414715719063545,
      "grad_norm": 5.178824424743652,
      "learning_rate": 4.684424754359366e-05,
      "loss": 0.5919,
      "step": 4050
    },
    {
      "epoch": 5.448160535117057,
      "grad_norm": 5.723593235015869,
      "learning_rate": 4.680058326723676e-05,
      "loss": 0.6309,
      "step": 4075
    },
    {
      "epoch": 5.481605351170568,
      "grad_norm": 3.6782853603363037,
      "learning_rate": 4.6756639624483564e-05,
      "loss": 0.6214,
      "step": 4100
    },
    {
      "epoch": 5.51505016722408,
      "grad_norm": 3.429457664489746,
      "learning_rate": 4.671241717845556e-05,
      "loss": 0.6161,
      "step": 4125
    },
    {
      "epoch": 5.548494983277592,
      "grad_norm": 3.361342668533325,
      "learning_rate": 4.6667916495847e-05,
      "loss": 0.6173,
      "step": 4150
    },
    {
      "epoch": 5.581939799331104,
      "grad_norm": 4.737313270568848,
      "learning_rate": 4.6623138146917656e-05,
      "loss": 0.6005,
      "step": 4175
    },
    {
      "epoch": 5.615384615384615,
      "grad_norm": 4.859488010406494,
      "learning_rate": 4.657808270548547e-05,
      "loss": 0.6045,
      "step": 4200
    },
    {
      "epoch": 5.648829431438127,
      "grad_norm": 4.753723621368408,
      "learning_rate": 4.653275074891923e-05,
      "loss": 0.6045,
      "step": 4225
    },
    {
      "epoch": 5.682274247491639,
      "grad_norm": 2.74959659576416,
      "learning_rate": 4.6487142858131165e-05,
      "loss": 0.6398,
      "step": 4250
    },
    {
      "epoch": 5.7157190635451505,
      "grad_norm": 4.409107208251953,
      "learning_rate": 4.6441259617569486e-05,
      "loss": 0.6427,
      "step": 4275
    },
    {
      "epoch": 5.749163879598662,
      "grad_norm": 3.4731826782226562,
      "learning_rate": 4.6395101615210934e-05,
      "loss": 0.5902,
      "step": 4300
    },
    {
      "epoch": 5.782608695652174,
      "grad_norm": 3.5438146591186523,
      "learning_rate": 4.634866944255319e-05,
      "loss": 0.6416,
      "step": 4325
    },
    {
      "epoch": 5.816053511705686,
      "grad_norm": 4.747596263885498,
      "learning_rate": 4.630196369460734e-05,
      "loss": 0.6309,
      "step": 4350
    },
    {
      "epoch": 5.849498327759197,
      "grad_norm": 4.248639106750488,
      "learning_rate": 4.625498496989026e-05,
      "loss": 0.6577,
      "step": 4375
    },
    {
      "epoch": 5.882943143812709,
      "grad_norm": 3.115157127380371,
      "learning_rate": 4.620773387041687e-05,
      "loss": 0.6261,
      "step": 4400
    },
    {
      "epoch": 5.916387959866221,
      "grad_norm": 4.050994873046875,
      "learning_rate": 4.616021100169251e-05,
      "loss": 0.6382,
      "step": 4425
    },
    {
      "epoch": 5.949832775919733,
      "grad_norm": 3.3800880908966064,
      "learning_rate": 4.6112416972705145e-05,
      "loss": 0.5959,
      "step": 4450
    },
    {
      "epoch": 5.983277591973244,
      "grad_norm": 3.7743782997131348,
      "learning_rate": 4.606435239591753e-05,
      "loss": 0.6122,
      "step": 4475
    },
    {
      "epoch": 6.016053511705685,
      "grad_norm": 3.111175298690796,
      "learning_rate": 4.6016017887259424e-05,
      "loss": 0.5684,
      "step": 4500
    },
    {
      "epoch": 6.016053511705685,
      "eval_loss": 1.140652060508728,
      "eval_runtime": 226.9172,
      "eval_samples_per_second": 6.584,
      "eval_steps_per_second": 0.824,
      "eval_wer": 92.10174029451139,
      "step": 4500
    },
    {
      "epoch": 6.049498327759197,
      "grad_norm": 4.173911094665527,
      "learning_rate": 4.596741406611963e-05,
      "loss": 0.5337,
      "step": 4525
    },
    {
      "epoch": 6.082943143812709,
      "grad_norm": 3.395742654800415,
      "learning_rate": 4.5918541555338114e-05,
      "loss": 0.5666,
      "step": 4550
    },
    {
      "epoch": 6.116387959866221,
      "grad_norm": 3.601740598678589,
      "learning_rate": 4.5869400981197984e-05,
      "loss": 0.5303,
      "step": 4575
    },
    {
      "epoch": 6.149832775919732,
      "grad_norm": 3.515469551086426,
      "learning_rate": 4.581999297341749e-05,
      "loss": 0.5114,
      "step": 4600
    },
    {
      "epoch": 6.183277591973244,
      "grad_norm": 4.14736270904541,
      "learning_rate": 4.577231027206855e-05,
      "loss": 0.5651,
      "step": 4625
    },
    {
      "epoch": 6.216722408026756,
      "grad_norm": 3.1968746185302734,
      "learning_rate": 4.572237993415461e-05,
      "loss": 0.5656,
      "step": 4650
    },
    {
      "epoch": 6.2501672240802675,
      "grad_norm": 4.0433149337768555,
      "learning_rate": 4.567218404662055e-05,
      "loss": 0.5385,
      "step": 4675
    },
    {
      "epoch": 6.28361204013378,
      "grad_norm": 3.651880979537964,
      "learning_rate": 4.562172325270806e-05,
      "loss": 0.5387,
      "step": 4700
    },
    {
      "epoch": 6.317056856187291,
      "grad_norm": 3.4431095123291016,
      "learning_rate": 4.557099819905352e-05,
      "loss": 0.5482,
      "step": 4725
    },
    {
      "epoch": 6.350501672240803,
      "grad_norm": 2.9754512310028076,
      "learning_rate": 4.55200095356797e-05,
      "loss": 0.5639,
      "step": 4750
    },
    {
      "epoch": 6.383946488294314,
      "grad_norm": 3.0725762844085693,
      "learning_rate": 4.546875791598742e-05,
      "loss": 0.5652,
      "step": 4775
    },
    {
      "epoch": 6.417391304347826,
      "grad_norm": 4.02362060546875,
      "learning_rate": 4.5417243996747214e-05,
      "loss": 0.5662,
      "step": 4800
    },
    {
      "epoch": 6.450836120401338,
      "grad_norm": 3.8192789554595947,
      "learning_rate": 4.536546843809084e-05,
      "loss": 0.6262,
      "step": 4825
    },
    {
      "epoch": 6.48428093645485,
      "grad_norm": 3.9059512615203857,
      "learning_rate": 4.531343190350292e-05,
      "loss": 0.5895,
      "step": 4850
    },
    {
      "epoch": 6.517725752508361,
      "grad_norm": 4.609775066375732,
      "learning_rate": 4.526113505981235e-05,
      "loss": 0.5926,
      "step": 4875
    },
    {
      "epoch": 6.551170568561873,
      "grad_norm": 3.6198649406433105,
      "learning_rate": 4.5208578577183815e-05,
      "loss": 0.5588,
      "step": 4900
    },
    {
      "epoch": 6.584615384615384,
      "grad_norm": 3.805800199508667,
      "learning_rate": 4.5155763129109144e-05,
      "loss": 0.5594,
      "step": 4925
    },
    {
      "epoch": 6.618060200668896,
      "grad_norm": 3.6913909912109375,
      "learning_rate": 4.510268939239875e-05,
      "loss": 0.5572,
      "step": 4950
    },
    {
      "epoch": 6.651505016722408,
      "grad_norm": 3.7621073722839355,
      "learning_rate": 4.50493580471729e-05,
      "loss": 0.5441,
      "step": 4975
    },
    {
      "epoch": 6.68494983277592,
      "grad_norm": 3.644568681716919,
      "learning_rate": 4.499576977685302e-05,
      "loss": 0.5259,
      "step": 5000
    },
    {
      "epoch": 6.68494983277592,
      "eval_loss": 1.139164686203003,
      "eval_runtime": 230.9754,
      "eval_samples_per_second": 6.468,
      "eval_steps_per_second": 0.81,
      "eval_wer": 91.36546184738957,
      "step": 5000
    }
  ],
  "logging_steps": 25,
  "max_steps": 22440,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.61407806144512e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
